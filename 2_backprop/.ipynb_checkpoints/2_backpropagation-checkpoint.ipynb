{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "23896409330ef70573fae3093456d53b",
     "grade": false,
     "grade_id": "cell-87195ccb7e06731c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Exercise 2. Backpropagation\n",
    "\n",
    "### Learning goals\n",
    "\n",
    "We will implement forward and backward computations in a simple neural network using pure numpy.\n",
    "This will help the students understand:\n",
    "* what is backpropagation\n",
    "* how to implement and test gradient computations\n",
    "* how backpropagation is implemented in packages like PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "skip_training = True  # Set this flag to True before validation and submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fe00604c3ac013b22df1df7f9c7f175c",
     "grade": true,
     "grade_id": "evaluation_settings",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# During grading, this cell sets skip_training to True\n",
    "# skip_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8dbf4fac7a0176a1e2fa7090854017bd",
     "grade": false,
     "grade_id": "cell-cafdead5e95c3773",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "069db8cb50b8d7512aa44f90a52fe40d",
     "grade": false,
     "grade_id": "cell-7473a37e84b4a136",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "When implementing analytical computations of gradients, it is useful to test the implementation by computing gradients numerically and comparing the results. The function below computes the gradients numerically, you may (but do not have to) use this function to test your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e0d8b858f8ca742338115aed5666fc63",
     "grade": false,
     "grade_id": "cell-14c19df006e22022",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This function computes the gradient of a given function fun at input x numperically, using the specified\n",
    "# perturbations eps\n",
    "def numerical_gradient(fun, x, eps=1e-4):\n",
    "    assert x.ndim <= 1, \"Only vector inputs are supported\"\n",
    "    e = np.zeros_like(x)\n",
    "    f = fun(x)\n",
    "    assert f.ndim <= 1, \"Only vector outputs are supported\"\n",
    "    gnum = np.zeros((f.size, x.size))\n",
    "    for i in range(len(x)):\n",
    "        e[:] = 0\n",
    "        e[i] = 1\n",
    "        f1, f2 = fun(x + e*eps), fun(x - e * eps)\n",
    "        gnum[:, i] = (f1 - f2) / (2 * eps)\n",
    "    return gnum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3ae43e2be4f66ded3f7493ed3ea3aeb4",
     "grade": false,
     "grade_id": "cell-cc9de3851f9d6d64",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Gradient of the loss\n",
    "\n",
    "Let us first compute the gradient of a simple loss function wrt to its inputs. Consider the mean-squared error loss:\n",
    "$$\n",
    "c = \\frac{1}{n} \\sum_{i=1}^n (y_i - t_i)^2\n",
    "$$\n",
    "where $y_i$ are the elements of an input vector $\\mathbf{y}$ and $t_i$ are the elements of the target vector $\\mathbf{t}$.\n",
    "\n",
    "In the code below, define a class that performs forward and backward computations of this loss function. The `backward` function should compute the gradient $\\frac{\\partial c}{\\partial \\mathbf{y}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e26360d15f781559becb30b881636c4b",
     "grade": false,
     "grade_id": "cell-32dc00fe264ae2cd",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class MSELoss:\n",
    "    def forward(self, y, target):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          y (array):      Inputs of the loss function (can be, e.g., an output of a neural network),\n",
    "                           shape (xsize,).\n",
    "          target (array): Targets, shape (xsize,).\n",
    "        \"\"\"\n",
    "        self.diff = diff = y - target  # Keep this for backward computations\n",
    "        c = np.sum(np.square(diff)) / diff.size\n",
    "        return c\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "          dy (array): Gradient of the MSE loss wrt the inputs, shape (xsize,).\n",
    "        \"\"\"\n",
    "        assert hasattr(self, 'diff'), \"Need to call forward() first\"\n",
    "        # YOUR CODE HERE\n",
    "        dy = 2 * self.diff / self.diff.size\n",
    "        return dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "78722c4f1a57b5bd754a021feffab2a2",
     "grade": false,
     "grade_id": "cell-978d99af4e668ea3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shapes seem to be ok.\n"
     ]
    }
   ],
   "source": [
    "# Let us first test that the shapes are correct\n",
    "y = np.random.randn(3)\n",
    "target = np.zeros(3)  # Dummy target\n",
    "loss = MSELoss()  # Create the loss\n",
    "loss_value = loss.forward(y, target)  # Do forward computations\n",
    "dy = loss.backward()  # Do backward computations\n",
    "assert dy.shape == y.shape, \"Bad shape of dy: dy.shape={}, y.shape={}\".format(dy.shape, y.shape)\n",
    "print(\"The shapes seem to be ok.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "29cd5ac2623632abb7614ec4e6e416b1",
     "grade": true,
     "grade_id": "MSELoss",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytical gradient:\n",
      " [-0.31022963 -0.37784975 -0.60668336]\n",
      "Numerical gradient:\n",
      " [-0.31022963 -0.37784975 -0.60668336]\n",
      "Success: Analytical and numerical results match.\n"
     ]
    }
   ],
   "source": [
    "# Let us compare the result to the gradient computed numerically\n",
    "dy = loss.backward()\n",
    "print('Analytical gradient:\\n', dy)\n",
    "dy_num = numerical_gradient(lambda y: loss.forward(y, target), y)\n",
    "print('Numerical gradient:\\n', dy_num[0])\n",
    "assert np.allclose(dy, dy_num), \\\n",
    "    'Analytical and numerical results differ.\\nAnalytical:\\n{}\\nNumerical:\\n{}'.format(dy, dy_num)\n",
    "print(\"Success: Analytical and numerical results match.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3)\n"
     ]
    }
   ],
   "source": [
    "print(dy_num.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2a03e143805589f6a2a38f88e7df4d76",
     "grade": false,
     "grade_id": "cell-e5a601312e669156",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "If the input `y` of the loss function is the output of a neural network, now we know how to compute the gradient of the loss wrt the network's outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d20325b55bd7059e28602b68fcdaa4cc",
     "grade": false,
     "grade_id": "cell-4528f00b8cfa797b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Backpropagation through activativation functions\n",
    "\n",
    "Let us propagate the gradients further through the network. Suppose that somewhere in the network, we have element-wise nonlinearities applied to input vector $\\mathbf{x}$ to produce outputs $\\mathbf{y}$:\n",
    "$$\n",
    "y_i = f(x_i).\n",
    "$$\n",
    "\n",
    "When we backpropagate through that block, we need to transform the gradients $\\frac{\\partial c}{\\partial \\mathbf{y}}$ wrt to the outputs into the gradients wrt the inputs $\\frac{\\partial c}{\\partial \\mathbf{x}}$. Your task is to implement the backward computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "42b36248de5859e9416ab13e869e2a1b",
     "grade": false,
     "grade_id": "cell-1c4411fa5a6bd2ee",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          x (array): Input of shape (xsize,).\n",
    "        \n",
    "        Returns:\n",
    "          y (array): Output of shape (xsize,).\n",
    "        \"\"\"\n",
    "        self.x = x  # Keep this for backward computations\n",
    "        return np.maximum(x, 0)\n",
    "\n",
    "    def backward(self, dy):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          dy (array): Gradient of a loss wrt outputs, shape (xsize,).\n",
    "        \n",
    "        Returns:\n",
    "          dx (array): Gradient of a loss wrt inputs, shape (xsize,).\n",
    "        \"\"\"\n",
    "        assert hasattr(self, 'x'), \"Need to call forward() first.\"\n",
    "        # YOUR CODE HERE\n",
    "        dydx = np.vectorize(lambda dy: 1 if dy > 0 else 0)\n",
    "        dx = dy * dydx(self.x)\n",
    "        #dx = np.array(list(map(dydx, dy)))\n",
    "        return dx\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c4e6d428f045d1fd21af6fbed36e36c8",
     "grade": false,
     "grade_id": "cell-ec3f1adedaa27bc5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shapes seem to be ok.\n"
     ]
    }
   ],
   "source": [
    "# Let us test the shapes\n",
    "x = np.random.randn(3)\n",
    "act_fn = ReLU()\n",
    "y = act_fn.forward(x)\n",
    "dy = np.ones(3)\n",
    "dx = act_fn.backward(dy)\n",
    "assert dx.shape == x.shape, \"Bad shape of dx: dx.shape={}, x.shape={}\".format(dx.shape, x.shape)\n",
    "print(\"The shapes seem to be ok.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c9278f8853d6bcbba050b31139e99906",
     "grade": false,
     "grade_id": "cell-964c8311d6ea791d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "It is recommended to compare analytical and numerical computations of the gradient. You can do this in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytical gradient:\n",
      " [0. 0. 1. 1.]\n",
      "Numerical gradient:\n",
      " [0. 0. 1. 1.]\n",
      "Success: Analytical and numerical results match.\n"
     ]
    }
   ],
   "source": [
    "# Let us compare the result to the gradient computed numerically\n",
    "x = np.array([-2,-0.8,0.3,5])\n",
    "act_fn = ReLU()\n",
    "y = act_fn.forward(x)\n",
    "dy = np.ones(len(x))\n",
    "dx = act_fn.backward(dy)\n",
    "print('Analytical gradient:\\n', dx)\n",
    "dx_num = np.diag(numerical_gradient(lambda a: act_fn.forward(a), x))\n",
    "print('Numerical gradient:\\n', dx_num)\n",
    "assert np.allclose(dx, dx_num), \\\n",
    "    'Analytical and numerical results differ.\\nAnalytical:\\n{}\\nNumerical:\\n{}'.format(dy, dy_num)\n",
    "print(\"Success: Analytical and numerical results match.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "62f50b10cc31669f3983b6b72bccd30b",
     "grade": true,
     "grade_id": "ReLU",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a cell used for grading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ae3ede920a17b00cfb02fb015564b24e",
     "grade": false,
     "grade_id": "cell-33e342a96c833d90",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Now your task to implement the `tanh` nonlinearity in the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6012719cc44ad6b4484cda8c54cbc8bd",
     "grade": false,
     "grade_id": "cell-253845a536af57eb",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class Tanh:\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          x (array): Input of shape (xsize,).\n",
    "        \n",
    "        Returns:\n",
    "          y (array): Output of shape (xsize,).\n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        y = np.tanh(x)\n",
    "        return y\n",
    "        \n",
    "\n",
    "    def backward(self, dy):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          dy (array): Gradient of a loss wrt outputs, shape (xsize,).\n",
    "        \n",
    "        Returns:\n",
    "          dx (array): Gradient of a loss wrt inputs, shape (xsize,).\n",
    "        \"\"\"\n",
    "        assert hasattr(self, 'x'), \"Need to call forward() first.\"\n",
    "        # YOUR CODE HERE\n",
    "        dx = dy * 1/np.cosh(self.x)**2\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "433e7f602b84b184e944b2fbbebb2dee",
     "grade": false,
     "grade_id": "cell-a67694f86298cbbc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shapes seem to be ok.\n"
     ]
    }
   ],
   "source": [
    "# Let us test the shapes\n",
    "x = np.random.randn(3)\n",
    "act_fn = Tanh()\n",
    "y = act_fn.forward(x)\n",
    "dy = np.ones(3)\n",
    "dx = act_fn.backward(dy)\n",
    "assert dx.shape == x.shape, \"Bad shape of dx: dx.shape={}, x.shape={}\".format(dx.shape, x.shape)\n",
    "print(\"The shapes seem to be ok.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6dd64d12c08242f1c73895f164ecb885",
     "grade": false,
     "grade_id": "cell-0ad8f605304b4bbe",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "It is recommended to compare analytical and numerical computations of the gradient. You can do this in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytical gradient:\n",
      " [7.06508249e-02 5.59055168e-01 9.15136962e-01 1.81583231e-04]\n",
      "Numerical gradient:\n",
      " [7.06508253e-02 5.59055168e-01 9.15136960e-01 1.81583232e-04]\n",
      "Success: Analytical and numerical results match.\n"
     ]
    }
   ],
   "source": [
    "# Let us compare the result to the gradient computed numerically\n",
    "act_fn = Tanh()\n",
    "#x = np.random.randn(5)\n",
    "x = np.array([-2,-0.8,0.3,5])\n",
    "y = act_fn.forward(x)\n",
    "dy = np.ones(len(x))\n",
    "\n",
    "dx = act_fn.backward(dy)\n",
    "print('Analytical gradient:\\n', dx)\n",
    "\n",
    "dx_num = np.diag(numerical_gradient(act_fn.forward, x))\n",
    "print('Numerical gradient:\\n', dx_num)\n",
    "assert np.allclose(dx, dx_num), \\\n",
    "    'Analytical and numerical results differ.\\nAnalytical:\\n{}\\nNumerical:\\n{}'.format(dy, dy_num)\n",
    "print(\"Success: Analytical and numerical results match.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "85087f0a3243317e513700f5b42158d3",
     "grade": true,
     "grade_id": "Tanh",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a cell used for grading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7f4dfcb15193de8d3558e70a19bcf59e",
     "grade": false,
     "grade_id": "cell-54b458a07135108a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Backpropagation through a linear layer\n",
    "\n",
    "Let us propagate the gradients through a linear layer. The linear layer implements forward computations:\n",
    "$$\n",
    "\\mathbf{y} = \\mathbf{W} \\mathbf{x} + \\mathbf{b}.\n",
    "$$\n",
    "\n",
    "In the backward pass, the linear layer receives the gradients wrt to the outputs $\\frac{\\partial c}{\\partial \\mathbf{y}}$ and it needs to compute:\n",
    "* the gradients wrt the layer parameters $\\mathbf{W}$ and $\\mathbf{b}$\n",
    "* the gradient $\\frac{\\partial c}{\\partial \\mathbf{x}}$ wrt the inputs.\n",
    "\n",
    "Let us implement the forward and the backward computations in the cell below.\n",
    "\n",
    "Hint:\n",
    "* We test the computations of $\\frac{\\partial c}{\\partial \\mathbf{W}}$ numerically. We recommend you to test $\\frac{\\partial c}{\\partial \\mathbf{x}}$ and $\\frac{\\partial c}{\\partial \\mathbf{b}}$ in a similar way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b9834af72c0d08a369fe1d6267750878",
     "grade": false,
     "grade_id": "cell-6a69458fa99a63e4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def linear_forward(x, W, b):\n",
    "    \"\"\"Forward computations in a linear layer:\n",
    "        y = W x + b\n",
    "\n",
    "    Args:\n",
    "      x (array): Input of shape (xsize,).\n",
    "      W (array): Weight matrix of shape (ysize, xsize).\n",
    "      b (array): Bias term of shape (ysize,).\n",
    "\n",
    "    Returns:\n",
    "      y (array): Output of shape (ysize,).\n",
    "    \"\"\"\n",
    "    return np.dot(W, x) + b\n",
    "\n",
    "def linear_backward(dy, x, W, b):\n",
    "    \"\"\"Backward computations in a linear layer.\n",
    "\n",
    "    Args:\n",
    "      dy (array): Gradient of a loss wrt outputs, shape (ysize,).\n",
    "      x (array): Input of shape (xsize,).\n",
    "      W (array): Weight matrix of shape (ysize, xsize).\n",
    "      b (array): Bias term of shape (ysize,).\n",
    "\n",
    "    Returns:\n",
    "      dx (array): Gradient wrt inputs (xsize,).\n",
    "      dW (array): Gradient wrt weight matrix W, shape (ysize, xsize).\n",
    "      db (array): Gradient wrt bias term b, shape (ysize,).\n",
    "    \"\"\"\n",
    "    dx = dy @ W\n",
    "    dW = dy[:, np.newaxis] @ x[:,np.newaxis].T\n",
    "    db = dy\n",
    "    \n",
    "    return dx, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shapes seem to be ok.\n",
      "Analytical gradient:\n",
      " [[-0.31014182 -0.50550579]\n",
      " [-0.31014182 -0.50550579]\n",
      " [-0.31014182 -0.50550579]]\n",
      "Numerical gradient:\n",
      " [[-0.31014182 -0.50550579]\n",
      " [-0.31014182 -0.50550579]\n",
      " [-0.31014182 -0.50550579]]\n",
      "Success: Analytical and numerical results match.\n"
     ]
    }
   ],
   "source": [
    "# Let us test your implementation\n",
    "x = np.random.randn(2)\n",
    "W = np.random.randn(3, 2)\n",
    "b = np.random.randn(3)\n",
    "\n",
    "# Test shapes\n",
    "y = linear_forward(x, W, b)\n",
    "dy = np.ones(3)\n",
    "dx, dW, db = linear_backward(dy, x, W, b)\n",
    "assert dx.shape == x.shape, \"Bad shape of dx: dx.shape={}, x.shape={}\".format(dx.shape, x.shape)\n",
    "assert dW.shape == W.shape, \"Bad shape of dW: dW.shape={}, W.shape={}\".format(dW.shape, W.shape)\n",
    "assert db.shape == b.shape, \"Bad shape of db: db.shape={}, b.shape={}\".format(db.shape, b.shape)\n",
    "print(\"The shapes seem to be ok.\")\n",
    "\n",
    "# Test gradient wrt W numerically\n",
    "print('Analytical gradient:\\n', dW)\n",
    "dW_num = numerical_gradient(lambda W: linear_forward(x, W.reshape(3, 2), b), W.flatten())\n",
    "dW_num = np.dot(dy.T, dW_num).reshape(3, 2)\n",
    "print('Numerical gradient:\\n', dW_num)\n",
    "assert np.allclose(dW, dW_num), \\\n",
    "    'Analytical and numerical results differ.\\nAnalytical:\\n{}\\nNumerical:\\n{}'.format(dW, dW_num)\n",
    "print(\"Success: Analytical and numerical results match.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4d6abfad4d6f439720deab9e8f68a227",
     "grade": false,
     "grade_id": "cell-ad8043675fc8e443",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "It is recommended to compare analytical and numerical computations of the gradients also wrt input `x` and bias term `b`. You can do this in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "03695197ffcf1916046ff84cd78dc866",
     "grade": true,
     "grade_id": "linear_Wb",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a cell used for grading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "34a15b184a905559bd35bd3b80b7d80e",
     "grade": true,
     "grade_id": "linear_x",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a cell used for grading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "77ad4427c68f83f95566659415ab1800",
     "grade": false,
     "grade_id": "cell-b121dbfaf4f713ca",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let us implement a class that represents a linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "867b9cfaf060043833c079685affdc17",
     "grade": false,
     "grade_id": "cell-cf7ed19f1e61067d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, in_features, out_features):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          in_features (int): Number of input features which should be equal to xsize.\n",
    "          out_features (out): Number of output features which should be equal to ysize.\n",
    "        \"\"\"\n",
    "        self.W = np.random.randn(out_features, in_features)\n",
    "        self.b = np.random.randn(out_features)\n",
    "\n",
    "        self.grad_W = None\n",
    "        self.grad_b = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          x (array): Input of shape (xsize,).\n",
    "        \n",
    "        Returns:\n",
    "          y (array): Output of shape (ysize,).\n",
    "        \"\"\"\n",
    "        self.x = x  # Keep this for backward computations\n",
    "        return linear_forward(x, self.W, self.b)\n",
    "\n",
    "    def backward(self, dy):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          dy (array): gradient of a loss wrt outputs, shape (ysize,).\n",
    "        \n",
    "        Returns:\n",
    "          dx (array): gradient of a loss wrt inputs, shape (xsize,).\n",
    "        \"\"\"\n",
    "        assert hasattr(self, 'x'), \"Need to call forward() first.\"\n",
    "        assert dy.size == self.W.shape[0]\n",
    "        dx, self.grad_W, self.grad_b = linear_backward(dy, self.x, self.W, self.b)\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "918e1153a1ee521088a496fc090caadf",
     "grade": false,
     "grade_id": "Linear",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# We can now create a linear layer, ...\n",
    "layer = Linear(in_features=3, out_features=2)\n",
    "\n",
    "# do forward computations ...\n",
    "x = np.random.randn(3)\n",
    "y = layer.forward(x)\n",
    "\n",
    "# and backward computations\n",
    "dy = np.ones(2)\n",
    "dx = layer.backward(dy)\n",
    "\n",
    "# We now have the gradients computed\n",
    "# wrt input x\n",
    "assert dx.shape == x.shape, \"Bad shape of dx: dx.shape={}, x.shape={}\".format(dx.shape, x.shape)\n",
    "# wrt weight matrix W\n",
    "assert layer.grad_W.shape == layer.W.shape, \\\n",
    "    \"Bad shape of grad_W: grad_W.shape={}, W.shape={}\".format(layer.grad_W.shape, layer.W.shape)\n",
    "# wrt bias term b\n",
    "assert layer.grad_b.shape == layer.b.shape, \\\n",
    "    \"Bad shape of grad_b: grad_b.shape={}, b.shape={}\".format(layer.grad_b.shape, layer.b.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d1b585a0ea445d39c226809cd8932fbd",
     "grade": false,
     "grade_id": "cell-b236e8b574736d45",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Define a multilayer perceptron and do full backpropagation\n",
    "\n",
    "Now let us define a multilayer perceptron and do backpropagation of the gradients from its outputs to its inputs. Your task is to implement forward and backward computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fe5d77028b0cafb93d1c1f0b2e2c46fe",
     "grade": false,
     "grade_id": "cell-410dc7c2f2257317",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, in_features, hidden_size1, hidden_size2, out_features):\n",
    "        \"\"\"Multilayer perceptron network with two hidden layers.\n",
    "        \n",
    "        Args:\n",
    "          in_features (int): Number of inputs.\n",
    "          hidden_size1 (int): Number of units in the first hidden layer.\n",
    "          hidden_size2 (int): Number of units in the second hidden layer.\n",
    "          out_features (int): Number of outputs.\n",
    "        \"\"\"\n",
    "        self.fc1 = Linear(in_features, hidden_size1)\n",
    "        self.activation_fn1 = Tanh()\n",
    "        self.fc2 = Linear(hidden_size1, hidden_size2)\n",
    "        self.activation_fn2 = Tanh()\n",
    "        self.fc3 = Linear(hidden_size2, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          x (array): Input of shape (xsize,).\n",
    "        \n",
    "        Returns:\n",
    "          y (array): Output of shape (ysize,).\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def backward(self, dy):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          dy (array): Gradient of a loss wrt outputs, shape (ysize,).\n",
    "        \n",
    "        Returns:\n",
    "          dx (array): Gradient of a loss wrt inputs, shape (xsize,).\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5373e65162507bf7cce4e18f08b3616c",
     "grade": false,
     "grade_id": "cell-5722d97030901c37",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Let us test your implementation\n",
    "x = np.random.randn(2)\n",
    "mlp = MLP(2, 10, 20, 5)\n",
    "y = mlp.forward(x)\n",
    "dy = np.ones(5)  # gradient of a loss function wrt MLP's outputs. We assume it to be all ones.\n",
    "dx = mlp.backward(dy)\n",
    "\n",
    "# Test shapes\n",
    "assert dx.shape == x.shape, \"Bad shape of dx: dx.shape={}, x.shape={}\".format(dx.shape, x.shape)\n",
    "print(\"The shapes seem to be ok.\")\n",
    "\n",
    "# Test gradient wrt x numerically\n",
    "print('Analytical gradient:\\n', dx)\n",
    "\n",
    "dx_num = numerical_gradient(lambda x: mlp.forward(x), x)\n",
    "dx_num = np.dot(dy.T, dx_num)\n",
    "print('Numerical gradient:\\n', dx_num)\n",
    "assert np.allclose(dx, dx_num), \\\n",
    "    'Analytical and numerical results differ.\\nAnalytical:\\n{}\\nNumerical:\\n{}'.format(dx, dx_num)\n",
    "print(\"Success: Analytical and numerical results match.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a0dfc0b8b2182773f26596185138016c",
     "grade": true,
     "grade_id": "MLP_forward",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a cell used for grading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "187ed1992c2c9d0b0089704c01f47ae7",
     "grade": true,
     "grade_id": "MLP_backward",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a cell used for grading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c4b0d80c867f6854c43538f7b02e771f",
     "grade": false,
     "grade_id": "cell-37f54f36740c6999",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Now we can connect the output of the MLP to the `MSELoss` defined previously and do full backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "133f7c24a937ea8365bac91333ef943e",
     "grade": false,
     "grade_id": "cell-4991fb5326051637",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "mlp = MLP(2, 10, 20, 5)\n",
    "loss = MSELoss()\n",
    "\n",
    "# Dummy inputs and targets\n",
    "x = np.random.randn(2)\n",
    "target = np.random.randn(5)\n",
    "\n",
    "# Forward computations\n",
    "y = mlp.forward(x)\n",
    "assert y.shape == target.shape, \"Bad shape of y: y.shape={}, target.shape={}\".format(y.shape, target.shape)\n",
    "c = loss.forward(y, target)\n",
    "\n",
    "# Backward computations\n",
    "dy = loss.backward()\n",
    "assert dy.shape == y.shape, \"Bad shape of dy: dy.shape={}, y.shape={}\".format(dy.shape, y.shape)\n",
    "mlp.backward(dy)\n",
    "\n",
    "# To update the parameters of the MLP, we can use the gradients, for example:\n",
    "print('The gradient wrt the weights of the first layer:\\n', mlp.fc1.grad_W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1c2da39386831619cc6cec874cb0ebfe",
     "grade": false,
     "grade_id": "cell-bdc01528f3086ebf",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Now you have an idea how automatic differentiation is implemented in packages like PyTorch.\n",
    "\n",
    "Note that the code above works only for propagating the error of a single data point (not of a mini-batch)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a03931460e8cdd4f3faf4285e2be6c43",
     "grade": false,
     "grade_id": "cell-b71f50dda717743d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Linear layer which supports batch processing\n",
    "\n",
    "In the cell below, we implement forward and backward computations for a linear layer which supports batch processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c66bd5eca3b95d39609c507c9e6433f7",
     "grade": false,
     "grade_id": "cell-225997cdacc80572",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def linear_forward_batch(x, W, b):\n",
    "    \"\"\"Forward computations in the linear layer:\n",
    "        y = W x + b\n",
    "\n",
    "    Args:\n",
    "      x (array): Inputs of shape (batch_size, xsize).\n",
    "      W (array): Weight matrix of shape (ysize, xsize).\n",
    "      b (array): Bias term of shape (ysize,).\n",
    "\n",
    "    Returns:\n",
    "      y (array): Outputs of shape (batch_size, ysize).\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "def linear_backward_batch(dy, x, W, b):\n",
    "    \"\"\"Backward computations in the linear layer.\n",
    "\n",
    "    Args:\n",
    "      dy (array): Gradient of a loss wrt outputs, shape (batch_size, ysize).\n",
    "      x (array): Input of shape (batch_size, xsize).\n",
    "      W (array): Weight matrix of shape (ysize, xsize).\n",
    "      b (array): Bias term of shape (ysize,).\n",
    "\n",
    "    Returns:\n",
    "      dx (array): Gradient of a loss wrt inputs, shape (batch_size, xsize).\n",
    "      dW (array): Gradient wrt weight matrix W, shape (ysize, xsize).\n",
    "      db (array): Gradient wrt bias term b, shape (ysize,).\n",
    "    \"\"\"\n",
    "    assert dy.ndim == 2 and dy.shape[1] == W.shape[0]\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e1e2dbb3498901cc8801deb5d86e55be",
     "grade": false,
     "grade_id": "cell-7d17ae70e7b767ae",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Let us test your implementation\n",
    "batch_size = 4\n",
    "x = np.random.randn(batch_size, 2)\n",
    "W = np.random.randn(3, 2)\n",
    "b = np.random.randn(3)\n",
    "\n",
    "# Test shapes\n",
    "y = linear_forward_batch(x, W, b)\n",
    "dy = np.ones((batch_size, 3))\n",
    "dx, dW, db = linear_backward_batch(dy, x, W, b)\n",
    "assert dx.shape == x.shape, \"Bad shape of dx: dx.shape={}, x.shape={}\".format(dx.shape, x.shape)\n",
    "assert dW.shape == W.shape, \"Bad shape of dW: dW.shape={}, W.shape={}\".format(dW.shape, W.shape)\n",
    "assert db.shape == b.shape, \"Bad shape of db: db.shape={}, b.shape={}\".format(db.shape, b.shape)\n",
    "print(\"The shapes seem to be ok.\")\n",
    "\n",
    "# Test gradient wrt W numerically\n",
    "print('Analytical gradient:\\n', dW)\n",
    "dW_num = numerical_gradient(lambda W: linear_forward_batch(x, W.reshape(3, 2), b).flatten(), W.flatten())\n",
    "dW_num = dW_num.reshape(y.shape + W.shape)\n",
    "expected = (dy[:, :, None, None] * dW_num).sum(axis=(0,1))\n",
    "print('Numerical gradient:\\n', expected)\n",
    "assert np.allclose(dW, expected), \\\n",
    "    'Analytical and numerical results differ.\\nAnalytical:\\n{}\\nNumerical:\\n{}'.format(dW, expected)\n",
    "print(\"Success: Analytical and numerical results match.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "844329725d0713d18437a3fd7aa66956",
     "grade": false,
     "grade_id": "cell-91d19db375ecd0d5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "It is recommended to compare analytical and numerical computations of the gradients also wrt input `x` and bias term `b`. You can do this in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "83b0d894c70ba2faf4fb0f6388513d4a",
     "grade": true,
     "grade_id": "linear_batch_Wb",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a cell used for grading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9db43c82543f74168a3614110fbdbfa3",
     "grade": true,
     "grade_id": "linear_batch_x",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a cell used for grading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3ad522c578be014878b1fca82fcb6947",
     "grade": false,
     "grade_id": "cell-96395e08fccd7200",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now we define a linear layer which supports batch processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "80dbc4dc458628b8ca2a3644e9d725bd",
     "grade": false,
     "grade_id": "cell-bc17d247e1fbdf32",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class LinearBatch:\n",
    "    def __init__(self, in_features, out_features):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          in_features (int): Number of input features which should be equal to xsize.\n",
    "          out_features (out): Number of output features which should be equal to ysize.\n",
    "        \"\"\"\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        # Uniform initialization of the weights\n",
    "        bound = 3 / np.sqrt(in_features)\n",
    "        self.W = np.random.uniform(-bound, bound, (out_features, in_features))\n",
    "        bound = 1 / np.sqrt(in_features)\n",
    "        self.b = np.random.uniform(-bound, bound, out_features)\n",
    "\n",
    "        self.grad_W = None\n",
    "        self.grad_b = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          x (array): Inputs of shape (batch_size, xsize).\n",
    "        \n",
    "        Returns:\n",
    "          y (array): Outputs of shape (batch_size, ysize).\n",
    "        \"\"\"\n",
    "        self.x = x  # Keep this for backward computations\n",
    "        return linear_forward_batch(x, self.W, self.b)\n",
    "\n",
    "    def backward(self, dy):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          dy (array): gradient of a loss wrt outputs, shape (batch_size, ysize).\n",
    "        \n",
    "        Returns:\n",
    "          dx (array): gradient of a loss wrt inputs, shape (batch_size, xsize).\n",
    "        \"\"\"\n",
    "        assert hasattr(self, 'x'), \"Need to call forward() first\"\n",
    "        assert dy.ndim == 2 and dy.shape[1] == self.W.shape[0]\n",
    "        dx, self.grad_W, self.grad_b = linear_backward_batch(dy, self.x, self.W, self.b)\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "68b8ac45c29d1b88a1d750aac91f9201",
     "grade": false,
     "grade_id": "cell-c8937d1050cb741e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# We can now create a linear layer which supports batch processing, ...\n",
    "layer = LinearBatch(in_features=3, out_features=2)\n",
    "\n",
    "# do forward computations ...\n",
    "batch_size = 4\n",
    "x = np.random.randn(batch_size, 3)\n",
    "y = layer.forward(x)\n",
    "\n",
    "# and backward computations\n",
    "dy = np.ones((batch_size, 2))\n",
    "dx = layer.backward(dy)\n",
    "\n",
    "# We now have the gradients computed\n",
    "# wrt input x\n",
    "assert dx.shape == x.shape, \"Bad shape of dx: dx.shape={}, x.shape={}\".format(dx.shape, x.shape)\n",
    "# wrt weight matrix W\n",
    "assert layer.grad_W.shape == layer.W.shape, \\\n",
    "    \"Bad shape of grad_W: grad_W.shape={}, W.shape={}\".format(layer.grad_W.shape, layer.W.shape)\n",
    "# wrt bias term b\n",
    "assert layer.grad_b.shape == layer.b.shape, \\\n",
    "    \"Bad shape of grad_b: grad_b.shape={}, b.shape={}\".format(layer.grad_b.shape, layer.b.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "85ee25397ab672279ad5ee62de149ca3",
     "grade": false,
     "grade_id": "cell-15e28241c83862b3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## MLP implementation which supports batch processing\n",
    "\n",
    "In the cell below, implement an MLP with two hidden layers and `Tanh` nonlinearity which supports batch processing. Use instances of classes `LinearBatch` and `Tanh` in your implementation. These instances should be attributes of class `MLPBatch` such as attribute `fc1` in the example below:\n",
    "```\n",
    "    def __init__(self, in_features, hidden_size1, hidden_size2, out_features):\n",
    "        self.fc1 = LinearBatch(...)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "62ab7b0ad9c5a689daaad1a082f81440",
     "grade": false,
     "grade_id": "cell-9282433f485eec3a",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class MLPBatch:\n",
    "    def __init__(self, in_features, hidden_size1, hidden_size2, out_features):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          in_features (int): Number of inputs which should be equal to xsize.\n",
    "          hidden_size1 (int): Number of units in the first hidden layer.\n",
    "          hidden_size2 (int): Number of units in the second hidden layer.\n",
    "          out_features (int): Number of outputs which should be equal to ysize.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          x (array): Input of shape [batch_size, xsize].\n",
    "        \n",
    "        Returns:\n",
    "          y (array): Output of shape [batch_size, ysize].\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def backward(self, dy):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          dy (array): Gradient of a loss wrt outputs (shape [batch_size, ysize]).\n",
    "        \n",
    "        Returns:\n",
    "          dx (array): Gradient of a loss wrt inputs (shape [batch_size, xsize]).\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "92e8cf169b794fafdc3ef295e21d2c10",
     "grade": false,
     "grade_id": "cell-07c80ef21983d673",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Let us test the shapes\n",
    "batch_size = 10\n",
    "x = np.random.randn(batch_size, 1)\n",
    "mlp_batch = MLPBatch(1, 10, 20, 1)\n",
    "y = mlp_batch.forward(x)\n",
    "\n",
    "dy = np.ones((batch_size, 1))  # Gradient of a loss function wrt MLP's outputs. We assume it to be all ones.\n",
    "dx = mlp_batch.backward(dy)\n",
    "assert dx.shape == x.shape, \"Bad shape of dx: dx.shape={}, x.shape={}\".format(dx.shape, x.shape)\n",
    "print(\"The shapes seem to be ok.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9faec4a226dd90342be2915a33f1fe07",
     "grade": true,
     "grade_id": "MLPBatch_numeric",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a cell used for grading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4e98cdef797c923df6e3d5118bf2e564",
     "grade": true,
     "grade_id": "MLPBatch",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a cell used for grading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a5b95c8c59605d0902557809dd6750e5",
     "grade": false,
     "grade_id": "cell-91d50afcfcbc324d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Let's create an MLP with random weights and compute the gradients wrt the inputs\n",
    "batch_size = 100\n",
    "x = np.linspace(-10, 10, batch_size)\n",
    "mlp_batch = MLPBatch(1, 10, 20, 1)\n",
    "y = mlp_batch.forward(x.reshape((batch_size, 1))).flatten()\n",
    "\n",
    "dy_dx = mlp_batch.backward(np.ones((batch_size, 1))).flatten()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, y)\n",
    "ax.plot(x, dy_dx)\n",
    "ax.grid(True)\n",
    "ax.legend(['y', 'dy_dx'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fbb865aabe7b04a79776e7f6fed6750f",
     "grade": false,
     "grade_id": "cell-d5136f1291f0c36a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "You can visually inspect whether the computations of the derivative seem correct.\n",
    "\n",
    "More importantly, we can compute the gradient of a loss wrt the parameters of the MLP. The gradients can be used to update the parameters using gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b6ec1aa9619dfef9169a6555a5480dec",
     "grade": false,
     "grade_id": "cell-0630dc5ad992327d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Training MLP network with backpropagation\n",
    "\n",
    "Now let us use our code to train an MLP network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2e0b1798b19d46a30fd9585db6867ec8",
     "grade": false,
     "grade_id": "cell-bb746d106b37391b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Let us generate toy data\n",
    "np.random.seed(2)\n",
    "x = np.random.randn(100, 1)\n",
    "x = np.sort(x, axis=0)\n",
    "targets = np.sin(x * 2 * np.pi / 3)\n",
    "targets = targets + 0.2 * np.random.randn(*targets.shape)\n",
    "\n",
    "# Plot the data\n",
    "fig, ax = plt.subplots(1)\n",
    "ax.plot(x, targets, '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7b3ec1f21b4b80cc52ad516dd1a0e92e",
     "grade": false,
     "grade_id": "cell-b5925a7912f3191c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# And train an MLP network using gradient descent\n",
    "from IPython import display\n",
    "\n",
    "mlp = MLPBatch(1, 10, 10, 1)  # Create MLP network\n",
    "loss = MSELoss()  # Create loss\n",
    "\n",
    "fig, ax = plt.subplots(1)\n",
    "ax.plot(x, targets, '.')\n",
    "learning_rate = 0.15\n",
    "n_epochs = 1 if skip_training else 200\n",
    "for i in range(n_epochs):\n",
    "    # Forward computations\n",
    "    y = mlp.forward(x)\n",
    "    c = loss.forward(y, targets)\n",
    "    \n",
    "    # Backward computations\n",
    "    dy = loss.backward()\n",
    "    dx = mlp.backward(dy)\n",
    "\n",
    "    # Gradient descent update\n",
    "    learning_rate *= 0.99  # Learning rate annealing\n",
    "    for module in mlp.__dict__.values():\n",
    "        if hasattr(module, 'W'):\n",
    "            module.W = module.W - module.grad_W * learning_rate\n",
    "            module.b = module.b - module.grad_b * learning_rate\n",
    "\n",
    "    ax.clear()\n",
    "    ax.plot(x, targets, '.')\n",
    "    ax.plot(x, y, 'r-')\n",
    "    ax.grid(True)\n",
    "    ax.set_title('Iteration %d/%d' % (i+1, n_epochs))\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(fig)\n",
    "    plt.pause(0.005)\n",
    "display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1af50694f01709d4bcf4a4cf1aa96041",
     "grade": false,
     "grade_id": "cell-841919d678edba70",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now you have implemented backpropagation and trained an MLP network using gradient descent.\n",
    "\n",
    "PyTorch makes it easier to create neural networks with different architectures and optimize its parameters using (variants of) gradient descent:\n",
    "* It contains multiple building blocks with forward and backward computations implemented.\n",
    "* It implements optimization methods that work well for neural networks.\n",
    "* Computations can be performed either on GPU or CPU using the same code."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
